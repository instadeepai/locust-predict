{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e5dc67",
   "metadata": {},
   "source": [
    "# Append History Data\n",
    "In this notebook, data from the `pseudo-absence generation` step is further processed to add 95 days history day for all temporal variables.\n",
    "\n",
    "To achieve this, the NASA dataset is written to a database, then queries are made to fetch data of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758247c",
   "metadata": {},
   "source": [
    "### Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as db\n",
    "from dateutil.relativedelta import relativedelta \n",
    "import glob\n",
    "import xarray\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "\n",
    "#path to NASA data\n",
    "NASA_basePath = '/mnt/disks/nasa/NASA' \n",
    "\n",
    "temporal_variables = [\n",
    "    'AvgSurfT_inst', \n",
    "    'Albedo_inst', \n",
    "    'SoilMoi0_10cm_inst', \n",
    "    'SoilMoi10_40cm_inst', \n",
    "    'SoilTMP0_10cm_inst', \n",
    "    'SoilTMP10_40cm_inst', \n",
    "    'Tveg_tavg', \n",
    "    'Wind_f_inst', \n",
    "    'Rainf_f_tavg', \n",
    "    'Tair_f_inst',\n",
    "    'Qair_f_inst', \n",
    "    'Psurf_f_inst' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2dc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo location gridding\n",
    "\n",
    "resx, resy = (0.25, 0.25)\n",
    "\n",
    "lat_to_bucket_id = lambda x: int((x+90)/resy)\n",
    "lon_to_bucket_id = lambda x: int((x+180)/resx)\n",
    "\n",
    "bucket_id_to_lat = lambda x: (x*resy) - 90\n",
    "bucket_id_to_lon = lambda x: (x*resx) - 180\n",
    "\n",
    "# date arithmetic\n",
    "\n",
    "def add_days(current_index, days):\n",
    "    return (pd.to_datetime(current_index[0]) + relativedelta(days=days), current_index[1], current_index[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up database and table\n",
    "\n",
    "table_name = \"nasa_noah_data\"\n",
    "engine = db.create_engine('sqlite:///NASA_GLDAS_NOAH025_3H.db')\n",
    "connection = engine.connect()\n",
    "metadata = db.MetaData()\n",
    "nasa_noah_data = db.Table(table_name, metadata, autoload=True, autoload_with=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do THIS ONLY ONCE\n",
    "# writing all NASA data to database\n",
    "\n",
    "db_start_date = pd.to_datetime(\"2000-01-01\")\n",
    "db_end_date = pd.to_datetime(\"2021-12-31\")\n",
    "\n",
    "current_date = db_start_date\n",
    "j = 0\n",
    "while current_date <= db_end_date:\n",
    "    if current_date.is_year_start:\n",
    "        print(current_date)\n",
    "    year, month, day = list(map(int, str(current_date.date()).split('-')))\n",
    "    base_name = f\"{NASA_basePath}/GLDAS_NOAH025_3H.A{year}{str(month).zfill(2)}\"\n",
    "    files_pattern = f\"{base_name}{str(day).zfill(2) }*.nc4\"\n",
    "    try:\n",
    "        data = xarray.open_mfdataset(files_pattern, parallel=True)\n",
    "        data = data.mean(dim=\"time\", skipna=True)\n",
    "        data = data[temporal_variables].to_dataframe().dropna(axis=0, how='all').reset_index()\n",
    "        data['lat_bucket_id'] = data['lat'].apply(lat_to_bucket_id)\n",
    "        data['lon_bucket_id'] = data['lon'].apply(lon_to_bucket_id)\n",
    "        data['year'] = year\n",
    "        data['month'] = month\n",
    "        data['day']  = day\n",
    "        data[\"date\"] = pd.to_datetime(data[['month', 'day', 'year']])\n",
    "        data.index += j\n",
    "        data.to_sql(table_name, engine, if_exists='append')\n",
    "        j = data.index[-1] + 1\n",
    "    except:\n",
    "        print(f\"Cannot read {current_date} data\")\n",
    "    current_date += relativedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8169fed",
   "metadata": {},
   "source": [
    "### Append History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a37afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to output of pseudo-absence generation notebook\n",
    "csv_filepath = '../data/train_val_random_geo.csv'\n",
    "data = pd.read_csv(csv_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45480b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# February has <= 28 days\n",
    "data.loc[((data['month']==2) & (data['day'] > 28)), 'day'] = 28\n",
    "data[\"date\"] = pd.to_datetime(data[['month', 'day', 'year']])\n",
    "data[\"observation_date\"] = data[\"date\"]\n",
    "data['lat_bucket_id'] = data['y'].apply(lat_to_bucket_id)\n",
    "data['lon_bucket_id'] = data['x'].apply(lon_to_bucket_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1133cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = data[['lat_bucket_id', 'lon_bucket_id']].describe()\n",
    "lat_min, lon_min = stats.loc['min']\n",
    "lat_max, lon_max = stats.loc['max']\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['date', 'lat_bucket_id', 'lon_bucket_id', 'x', 'y', 'presence', 'year', 'month', 'day', 'clay_0.5cm_mean', 'clay_5.15cm_mean', 'sand_0.5cm_mean', 'sand_5.15cm_mean', 'silt_0.5cm_mean', 'silt_5.15cm_mean', 'observation_date']]\n",
    "data = data.set_index(['date', 'lat_bucket_id', 'lon_bucket_id'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b706ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_history_parallel(year, data):\n",
    "    # February has <= 28 days\n",
    "    data.loc[((data['month']==2) & (data['day'] > 28)), 'day'] = 28\n",
    "    data[\"date\"] = pd.to_datetime(data[['month', 'day', 'year']])\n",
    "    data[\"observation_date\"] = data[\"date\"]\n",
    "    data['lat_bucket_id'] = data['y'].apply(lat_to_bucket_id)\n",
    "    data['lon_bucket_id'] = data['x'].apply(lon_to_bucket_id)\n",
    "    data = data[['date', 'lat_bucket_id', 'lon_bucket_id', 'x', 'y', 'presence', 'method', 'year', 'month', 'day', 'clay_0.5cm_mean', 'clay_5.15cm_mean', 'sand_0.5cm_mean', 'sand_5.15cm_mean', 'silt_0.5cm_mean', 'silt_5.15cm_mean', 'observation_date']]\n",
    "    data = data.set_index(['date', 'lat_bucket_id', 'lon_bucket_id'])\n",
    "    \n",
    "\n",
    "    start_date = str((relativedelta(days=-95) + pd.to_datetime(f\"{year}-01-01\")).date())\n",
    "    end_date = str((relativedelta(days=365) + pd.to_datetime(f\"{year}-01-01\")).date())\n",
    "    print(f\"Year -> From: {start_date}, To: {end_date}\")\n",
    "    query = db.select([nasa_noah_data]).where(db.and_(\n",
    "        nasa_noah_data.columns.date >= start_date, \n",
    "        nasa_noah_data.columns.date <= end_date,\n",
    "        nasa_noah_data.columns.lat_bucket_id >= 347,\n",
    "        nasa_noah_data.columns.lat_bucket_id <= 504,\n",
    "        nasa_noah_data.columns.lon_bucket_id >= 619,\n",
    "        nasa_noah_data.columns.lon_bucket_id <= 924,\n",
    "    ))\n",
    "    query_result = pd.read_sql_query(query, engine).set_index(['date', 'lat_bucket_id', 'lon_bucket_id'])\n",
    "    subset = data[data['year'] == year]\n",
    "    for days in range(0, 96):\n",
    "        indices = subset['observation_date'].index.map(lambda row: add_days(row, days=-days))\n",
    "        subset_day_x = query_result.reindex(indices)\n",
    "        for variable in temporal_variables:\n",
    "            subset[f\"{variable}_{days}\"] = list(subset_day_x[variable])\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15fd596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to output of pseudo-absence generation notebook\n",
    "filepaths = [\n",
    "    '../data/train_val_random_geo.csv', \n",
    "]\n",
    "\n",
    "for filepath in filepaths:\n",
    "    data = pd.read_csv(filepath)\n",
    "    unique_years = data['year'].unique()\n",
    "\n",
    "    with mp.Pool(3) as p:\n",
    "        results = p.map(partial(add_history_parallel, data=data), unique_years)\n",
    "\n",
    "    output = pd.concat(results).reset_index(drop=True)\n",
    "    output.to_csv(f\"{os.path.splitext(filepath)[0]}_full.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
