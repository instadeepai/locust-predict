{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e5dc67",
   "metadata": {},
   "source": [
    "# Append History Data\n",
    "In this notebook, data from the `pseudo-absence generation` step is further processed to add 95 days history day for all temporal variables.\n",
    "\n",
    "To achieve this, the NASA dataset is written to a database, then queries are made to fetch data of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758247c",
   "metadata": {},
   "source": [
    "### Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c2bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as db\n",
    "from dateutil.relativedelta import relativedelta \n",
    "import glob\n",
    "import xarray\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c918562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "\n",
    "#path to NASA data\n",
    "NASA_basePath = '/mnt/disks/nasa/NASA' \n",
    "\n",
    "temporal_variables = [\n",
    "    'AvgSurfT_inst', \n",
    "    'Albedo_inst', \n",
    "    'SoilMoi0_10cm_inst', \n",
    "    'SoilMoi10_40cm_inst', \n",
    "    'SoilTMP0_10cm_inst', \n",
    "    'SoilTMP10_40cm_inst', \n",
    "    'Tveg_tavg', \n",
    "    'Wind_f_inst', \n",
    "    'Rainf_f_tavg', \n",
    "    'Tair_f_inst',\n",
    "    'Qair_f_inst', \n",
    "    'Psurf_f_inst' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d2dc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo location gridding\n",
    "\n",
    "resx, resy = (0.25, 0.25)\n",
    "\n",
    "lat_to_bucket_id = lambda x: int((x+90)/resy)\n",
    "lon_to_bucket_id = lambda x: int((x+180)/resx)\n",
    "\n",
    "bucket_id_to_lat = lambda x: (x*resy) - 90\n",
    "bucket_id_to_lon = lambda x: (x*resx) - 180\n",
    "\n",
    "# date arithmetic\n",
    "\n",
    "def add_days(current_index, days):\n",
    "    return (pd.to_datetime(current_index[0]) + relativedelta(days=days), current_index[1], current_index[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b4f9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up database and table\n",
    "\n",
    "table_name = \"nasa_noah_data\"\n",
    "engine = db.create_engine('sqlite:///NASA_GLDAS_NOAH025_3H.db')\n",
    "connection = engine.connect()\n",
    "metadata = db.MetaData()\n",
    "nasa_noah_data = db.Table(table_name, metadata, autoload=True, autoload_with=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5fa971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do THIS ONLY ONCE\n",
    "# writing all NASA data to database\n",
    "\n",
    "db_start_date = pd.to_datetime(\"2000-01-01\")\n",
    "db_end_date = pd.to_datetime(\"2021-12-31\")\n",
    "\n",
    "current_date = db_start_date\n",
    "j = 0\n",
    "while current_date <= db_end_date:\n",
    "    if current_date.is_year_start:\n",
    "        print(current_date)\n",
    "    year, month, day = list(map(int, str(current_date.date()).split('-')))\n",
    "    base_name = f\"{NASA_basePath}/GLDAS_NOAH025_3H.A{year}{str(month).zfill(2)}\"\n",
    "    files_pattern = f\"{base_name}{str(day).zfill(2) }*.nc4\"\n",
    "    try:\n",
    "        data = xarray.open_mfdataset(files_pattern, parallel=True)\n",
    "        data = data.mean(dim=\"time\", skipna=True)\n",
    "        data = data[temporal_variables].to_dataframe().dropna(axis=0, how='all').reset_index()\n",
    "        data['lat_bucket_id'] = data['lat'].apply(lat_to_bucket_id)\n",
    "        data['lon_bucket_id'] = data['lon'].apply(lon_to_bucket_id)\n",
    "        data['year'] = year\n",
    "        data['month'] = month\n",
    "        data['day']  = day\n",
    "        data[\"date\"] = pd.to_datetime(data[['month', 'day', 'year']])\n",
    "        data.index += j\n",
    "        data.to_sql(table_name, engine, if_exists='append')\n",
    "        j = data.index[-1] + 1\n",
    "    except:\n",
    "        print(f\"Cannot read {current_date} data\")\n",
    "    current_date += relativedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8169fed",
   "metadata": {},
   "source": [
    "### Append History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a37afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filepath = 'train_val_random_v0.csv'\n",
    "data = pd.read_csv(csv_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45480b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# February has <= 28 days\n",
    "data.loc[((data['month']==2) & (data['day'] > 28)), 'day'] = 28\n",
    "data[\"date\"] = pd.to_datetime(data[['month', 'day', 'year']])\n",
    "data[\"observation_date\"] = data[\"date\"]\n",
    "data['lat_bucket_id'] = data['y'].apply(lat_to_bucket_id)\n",
    "data['lon_bucket_id'] = data['x'].apply(lon_to_bucket_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd1133cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat_bucket_id</th>\n",
       "      <th>lon_bucket_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16159.000000</td>\n",
       "      <td>16159.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>435.691627</td>\n",
       "      <td>680.946284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.376503</td>\n",
       "      <td>20.807735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>401.000000</td>\n",
       "      <td>652.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>430.000000</td>\n",
       "      <td>665.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>436.000000</td>\n",
       "      <td>673.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>442.000000</td>\n",
       "      <td>695.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>468.000000</td>\n",
       "      <td>736.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lat_bucket_id  lon_bucket_id\n",
       "count   16159.000000   16159.000000\n",
       "mean      435.691627     680.946284\n",
       "std        11.376503      20.807735\n",
       "min       401.000000     652.000000\n",
       "25%       430.000000     665.000000\n",
       "50%       436.000000     673.000000\n",
       "75%       442.000000     695.000000\n",
       "max       468.000000     736.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = data[['lat_bucket_id', 'lon_bucket_id']].describe()\n",
    "lat_min, lon_min = stats.loc['min']\n",
    "lat_max, lon_max = stats.loc['max']\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bcf4ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>presence</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>clay_0.5cm_mean</th>\n",
       "      <th>clay_5.15cm_mean</th>\n",
       "      <th>sand_0.5cm_mean</th>\n",
       "      <th>sand_5.15cm_mean</th>\n",
       "      <th>silt_0.5cm_mean</th>\n",
       "      <th>silt_5.15cm_mean</th>\n",
       "      <th>observation_date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>lat_bucket_id</th>\n",
       "      <th>lon_bucket_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-11</th>\n",
       "      <th>442</th>\n",
       "      <th>658</th>\n",
       "      <td>-15.455833</td>\n",
       "      <td>20.581944</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.134760</td>\n",
       "      <td>0.140545</td>\n",
       "      <td>0.651147</td>\n",
       "      <td>0.647941</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>0.211528</td>\n",
       "      <td>2000-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-29</th>\n",
       "      <th>461</th>\n",
       "      <th>676</th>\n",
       "      <td>-10.933333</td>\n",
       "      <td>25.383333</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.153232</td>\n",
       "      <td>0.615415</td>\n",
       "      <td>0.621584</td>\n",
       "      <td>0.223054</td>\n",
       "      <td>0.225189</td>\n",
       "      <td>2000-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <th>437</th>\n",
       "      <th>655</th>\n",
       "      <td>-16.045278</td>\n",
       "      <td>19.471111</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.178887</td>\n",
       "      <td>0.182475</td>\n",
       "      <td>0.554956</td>\n",
       "      <td>0.554456</td>\n",
       "      <td>0.229309</td>\n",
       "      <td>0.226969</td>\n",
       "      <td>2000-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02</th>\n",
       "      <th>435</th>\n",
       "      <th>660</th>\n",
       "      <td>-14.760833</td>\n",
       "      <td>18.966389</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.195740</td>\n",
       "      <td>0.199793</td>\n",
       "      <td>0.574979</td>\n",
       "      <td>0.565655</td>\n",
       "      <td>0.229144</td>\n",
       "      <td>0.234419</td>\n",
       "      <td>2000-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-17</th>\n",
       "      <th>442</th>\n",
       "      <th>658</th>\n",
       "      <td>-15.440278</td>\n",
       "      <td>20.643056</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>0.140512</td>\n",
       "      <td>0.653609</td>\n",
       "      <td>0.650924</td>\n",
       "      <td>0.211982</td>\n",
       "      <td>0.208577</td>\n",
       "      <td>2000-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-08</th>\n",
       "      <th>418</th>\n",
       "      <th>687</th>\n",
       "      <td>-8.125000</td>\n",
       "      <td>14.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.154982</td>\n",
       "      <td>0.164436</td>\n",
       "      <td>0.665226</td>\n",
       "      <td>0.657830</td>\n",
       "      <td>0.179781</td>\n",
       "      <td>0.177740</td>\n",
       "      <td>2014-10-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-05</th>\n",
       "      <th>438</th>\n",
       "      <th>684</th>\n",
       "      <td>-8.875000</td>\n",
       "      <td>19.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.230420</td>\n",
       "      <td>0.228502</td>\n",
       "      <td>0.605109</td>\n",
       "      <td>0.604785</td>\n",
       "      <td>0.164489</td>\n",
       "      <td>0.166697</td>\n",
       "      <td>2014-10-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-04</th>\n",
       "      <th>455</th>\n",
       "      <th>685</th>\n",
       "      <td>-8.625000</td>\n",
       "      <td>23.875000</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.285872</td>\n",
       "      <td>0.279326</td>\n",
       "      <td>0.496537</td>\n",
       "      <td>0.500304</td>\n",
       "      <td>0.217598</td>\n",
       "      <td>0.220379</td>\n",
       "      <td>2014-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-26</th>\n",
       "      <th>408</th>\n",
       "      <th>701</th>\n",
       "      <td>-4.625000</td>\n",
       "      <td>12.125000</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>0.073982</td>\n",
       "      <td>0.076866</td>\n",
       "      <td>0.227040</td>\n",
       "      <td>0.226097</td>\n",
       "      <td>0.153879</td>\n",
       "      <td>0.154353</td>\n",
       "      <td>2014-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-12</th>\n",
       "      <th>435</th>\n",
       "      <th>664</th>\n",
       "      <td>-13.767222</td>\n",
       "      <td>18.923333</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.220910</td>\n",
       "      <td>0.234178</td>\n",
       "      <td>0.575341</td>\n",
       "      <td>0.566252</td>\n",
       "      <td>0.203697</td>\n",
       "      <td>0.199504</td>\n",
       "      <td>2014-12-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16159 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                x          y  presence  year  \\\n",
       "date       lat_bucket_id lon_bucket_id                                         \n",
       "2000-01-11 442           658           -15.455833  20.581944         1  2000   \n",
       "2000-01-29 461           676           -10.933333  25.383333         1  2000   \n",
       "2000-01-04 437           655           -16.045278  19.471111         1  2000   \n",
       "2000-01-02 435           660           -14.760833  18.966389         1  2000   \n",
       "2000-01-17 442           658           -15.440278  20.643056         1  2000   \n",
       "...                                           ...        ...       ...   ...   \n",
       "2014-10-08 418           687            -8.125000  14.625000         0  2014   \n",
       "2014-10-05 438           684            -8.875000  19.625000         0  2014   \n",
       "2014-10-04 455           685            -8.625000  23.875000         0  2014   \n",
       "2014-10-26 408           701            -4.625000  12.125000         0  2014   \n",
       "2014-12-12 435           664           -13.767222  18.923333         1  2014   \n",
       "\n",
       "                                        month  day  clay_0.5cm_mean  \\\n",
       "date       lat_bucket_id lon_bucket_id                                \n",
       "2000-01-11 442           658                1   11         0.134760   \n",
       "2000-01-29 461           676                1   29         0.161538   \n",
       "2000-01-04 437           655                1    4         0.178887   \n",
       "2000-01-02 435           660                1    2         0.195740   \n",
       "2000-01-17 442           658                1   17         0.134416   \n",
       "...                                       ...  ...              ...   \n",
       "2014-10-08 418           687               10    8         0.154982   \n",
       "2014-10-05 438           684               10    5         0.230420   \n",
       "2014-10-04 455           685               10    4         0.285872   \n",
       "2014-10-26 408           701               10   26         0.073982   \n",
       "2014-12-12 435           664               12   12         0.220910   \n",
       "\n",
       "                                        clay_5.15cm_mean  sand_0.5cm_mean  \\\n",
       "date       lat_bucket_id lon_bucket_id                                      \n",
       "2000-01-11 442           658                    0.140545         0.651147   \n",
       "2000-01-29 461           676                    0.153232         0.615415   \n",
       "2000-01-04 437           655                    0.182475         0.554956   \n",
       "2000-01-02 435           660                    0.199793         0.574979   \n",
       "2000-01-17 442           658                    0.140512         0.653609   \n",
       "...                                                  ...              ...   \n",
       "2014-10-08 418           687                    0.164436         0.665226   \n",
       "2014-10-05 438           684                    0.228502         0.605109   \n",
       "2014-10-04 455           685                    0.279326         0.496537   \n",
       "2014-10-26 408           701                    0.076866         0.227040   \n",
       "2014-12-12 435           664                    0.234178         0.575341   \n",
       "\n",
       "                                        sand_5.15cm_mean  silt_0.5cm_mean  \\\n",
       "date       lat_bucket_id lon_bucket_id                                      \n",
       "2000-01-11 442           658                    0.647941         0.214100   \n",
       "2000-01-29 461           676                    0.621584         0.223054   \n",
       "2000-01-04 437           655                    0.554456         0.229309   \n",
       "2000-01-02 435           660                    0.565655         0.229144   \n",
       "2000-01-17 442           658                    0.650924         0.211982   \n",
       "...                                                  ...              ...   \n",
       "2014-10-08 418           687                    0.657830         0.179781   \n",
       "2014-10-05 438           684                    0.604785         0.164489   \n",
       "2014-10-04 455           685                    0.500304         0.217598   \n",
       "2014-10-26 408           701                    0.226097         0.153879   \n",
       "2014-12-12 435           664                    0.566252         0.203697   \n",
       "\n",
       "                                        silt_5.15cm_mean observation_date  \n",
       "date       lat_bucket_id lon_bucket_id                                     \n",
       "2000-01-11 442           658                    0.211528       2000-01-11  \n",
       "2000-01-29 461           676                    0.225189       2000-01-29  \n",
       "2000-01-04 437           655                    0.226969       2000-01-04  \n",
       "2000-01-02 435           660                    0.234419       2000-01-02  \n",
       "2000-01-17 442           658                    0.208577       2000-01-17  \n",
       "...                                                  ...              ...  \n",
       "2014-10-08 418           687                    0.177740       2014-10-08  \n",
       "2014-10-05 438           684                    0.166697       2014-10-05  \n",
       "2014-10-04 455           685                    0.220379       2014-10-04  \n",
       "2014-10-26 408           701                    0.154353       2014-10-26  \n",
       "2014-12-12 435           664                    0.199504       2014-12-12  \n",
       "\n",
       "[16159 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['date', 'lat_bucket_id', 'lon_bucket_id', 'x', 'y', 'presence', 'year', 'month', 'day', 'clay_0.5cm_mean', 'clay_5.15cm_mean', 'sand_0.5cm_mean', 'sand_5.15cm_mean', 'silt_0.5cm_mean', 'silt_5.15cm_mean', 'observation_date']]\n",
    "data = data.set_index(['date', 'lat_bucket_id', 'lon_bucket_id'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43b706ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_history_parallel(year, data):\n",
    "    # February has <= 28 days\n",
    "    data.loc[((data['month']==2) & (data['day'] > 28)), 'day'] = 28\n",
    "    data[\"date\"] = pd.to_datetime(data[['month', 'day', 'year']])\n",
    "    data[\"observation_date\"] = data[\"date\"]\n",
    "    data['lat_bucket_id'] = data['y'].apply(lat_to_bucket_id)\n",
    "    data['lon_bucket_id'] = data['x'].apply(lon_to_bucket_id)\n",
    "    data = data[['date', 'lat_bucket_id', 'lon_bucket_id', 'x', 'y', 'presence', 'method', 'year', 'month', 'day', 'clay_0.5cm_mean', 'clay_5.15cm_mean', 'sand_0.5cm_mean', 'sand_5.15cm_mean', 'silt_0.5cm_mean', 'silt_5.15cm_mean', 'observation_date']]\n",
    "    data = data.set_index(['date', 'lat_bucket_id', 'lon_bucket_id'])\n",
    "    \n",
    "\n",
    "    start_date = str((relativedelta(days=-95) + pd.to_datetime(f\"{year}-01-01\")).date())\n",
    "    end_date = str((relativedelta(days=365) + pd.to_datetime(f\"{year}-01-01\")).date())\n",
    "    print(f\"Year -> From: {start_date}, To: {end_date}\")\n",
    "    query = db.select([nasa_noah_data]).where(db.and_(\n",
    "        nasa_noah_data.columns.date >= start_date, \n",
    "        nasa_noah_data.columns.date <= end_date,\n",
    "        nasa_noah_data.columns.lat_bucket_id >= 347,\n",
    "        nasa_noah_data.columns.lat_bucket_id <= 504,\n",
    "        nasa_noah_data.columns.lon_bucket_id >= 619,\n",
    "        nasa_noah_data.columns.lon_bucket_id <= 924,\n",
    "    ))\n",
    "    query_result = pd.read_sql_query(query, engine).set_index(['date', 'lat_bucket_id', 'lon_bucket_id'])\n",
    "    subset = data[data['year'] == year]\n",
    "    for days in range(0, 96):\n",
    "        indices = subset['observation_date'].index.map(lambda row: add_days(row, days=-days))\n",
    "        subset_day_x = query_result.reindex(indices)\n",
    "        for variable in temporal_variables:\n",
    "            subset[f\"{variable}_{days}\"] = list(subset_day_x[variable])\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a15fd596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year -> From: 1999-09-28, To: 2000-12-31\n",
      "Year -> From: 2001-09-28, To: 2003-01-01\n",
      "Year -> From: 2003-09-28, To: 2004-12-31\n",
      "Year -> From: 2000-09-28, To: 2002-01-01\n",
      "Year -> From: 2002-09-28, To: 2004-01-01\n",
      "Year -> From: 2004-09-28, To: 2006-01-01\n",
      "Year -> From: 2005-09-28, To: 2007-01-01\n",
      "Year -> From: 2007-09-28, To: 2008-12-31\n",
      "Year -> From: 2009-09-28, To: 2011-01-01\n",
      "Year -> From: 2006-09-28, To: 2008-01-01\n",
      "Year -> From: 2008-09-28, To: 2010-01-01\n",
      "Year -> From: 2010-09-28, To: 2012-01-01\n",
      "Year -> From: 2011-09-28, To: 2012-12-31\n",
      "Year -> From: 2013-09-28, To: 2015-01-01\n",
      "Year -> From: 2012-09-28, To: 2014-01-01\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\n",
    "#     'v4_2/test_gen_v4_2.csv', \n",
    "    'v4_2/train_val_gen_v4_2.csv', \n",
    "#     'v3/test_ep_random_v3.csv', \n",
    "#     'v3/train_val_ep_random_v3.csv', \n",
    "#     'train_val_ep_kmeans_v2.csv', \n",
    "#     'test_ep_kmeans_v2.csv'\n",
    "]\n",
    "\n",
    "for filepath in filepaths:\n",
    "    data = pd.read_csv(filepath)\n",
    "    unique_years = data['year'].unique()\n",
    "\n",
    "    with mp.Pool(3) as p:\n",
    "        results = p.map(partial(add_history_parallel, data=data), unique_years)\n",
    "\n",
    "    output = pd.concat(results).reset_index(drop=True)\n",
    "    output.to_csv(f\"{os.path.splitext(filepath)[0]}_full.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
